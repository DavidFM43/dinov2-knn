{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class DownsampleD(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleD, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.conv = nn.Conv2d(nIn, nOut, kernel_size=2, stride=stride, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class ResNetBasicblock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResNetBasicblock, self).__init__()\n",
    "\n",
    "        self.conv_a = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn_a = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv_b = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_b = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        basicblock = self.conv_a(x)\n",
    "        basicblock = self.bn_a(basicblock)\n",
    "        basicblock = F.relu(basicblock, inplace=True)\n",
    "\n",
    "        basicblock = self.conv_b(basicblock)\n",
    "        basicblock = self.bn_b(basicblock)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return F.relu(residual + basicblock, inplace=True)\n",
    "\n",
    "class CifarResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet optimized for the Cifar Dataset, as specified in\n",
    "    https://arxiv.org/abs/1512.03385.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, depth, channels=3):\n",
    "        super(CifarResNet, self).__init__()\n",
    "\n",
    "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
    "        layer_blocks = (depth - 2) // 6\n",
    "\n",
    "        self.conv_1_3x3 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.stage_1 = self._make_layer(block, 16, layer_blocks, 1)\n",
    "        self.stage_2 = self._make_layer(block, 32, layer_blocks, 2)\n",
    "        self.stage_3 = self._make_layer(block, 64, layer_blocks, 2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.out_dim = 64 * block.expansion\n",
    "        self.fc = nn.Linear(64 * block.expansion, 10)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = DownsampleD(self.inplanes, planes * block.expansion, stride)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_3x3(x)  # [bs, 16, 32, 32]\n",
    "        x = F.relu(self.bn_1(x), inplace=True)\n",
    "\n",
    "        x_1 = self.stage_1(x)  # [bs, 16, 32, 32]\n",
    "        x_2 = self.stage_2(x_1)  # [bs, 32, 16, 16]\n",
    "        x_3 = self.stage_3(x_2)  # [bs, 64, 8, 8]\n",
    "\n",
    "        pooled = self.avgpool(x_3)  # [bs, 64, 1, 1]\n",
    "        features = pooled.view(pooled.size(0), -1)  # [bs, 64]\n",
    "\n",
    "        return {\n",
    "            'fmaps': [x_1, x_2, x_3],\n",
    "            'features': features\n",
    "        }\n",
    "    \n",
    "    def freeze_stage(self, stage):\n",
    "        \"\"\"\n",
    "        Freeze the parameters of a given stage (1, 2, or 3).\n",
    "        \"\"\"\n",
    "        assert stage in [1, 2, 3], \"Stage should be 1, 2, or 3\"\n",
    "        \n",
    "        if stage == 1:\n",
    "            for param in self.stage_1.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif stage == 2:\n",
    "            for param in self.stage_1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.stage_2.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif stage == 3:\n",
    "            for param in self.stage_1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.stage_2.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.stage_3.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def print_freeze_status(self):\n",
    "        \"\"\"\n",
    "        Print the freeze status of each stage.\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                print(f'{name} is frozen')\n",
    "            else:\n",
    "                print(f'{name} is not frozen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de uso:\n",
    "model = CifarResNet(ResNetBasicblock, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1_3x3.weight is not frozen\n",
      "bn_1.weight is not frozen\n",
      "bn_1.bias is not frozen\n",
      "stage_1.0.conv_a.weight is frozen\n",
      "stage_1.0.bn_a.weight is frozen\n",
      "stage_1.0.bn_a.bias is frozen\n",
      "stage_1.0.conv_b.weight is frozen\n",
      "stage_1.0.bn_b.weight is frozen\n",
      "stage_1.0.bn_b.bias is frozen\n",
      "stage_1.1.conv_a.weight is frozen\n",
      "stage_1.1.bn_a.weight is frozen\n",
      "stage_1.1.bn_a.bias is frozen\n",
      "stage_1.1.conv_b.weight is frozen\n",
      "stage_1.1.bn_b.weight is frozen\n",
      "stage_1.1.bn_b.bias is frozen\n",
      "stage_1.2.conv_a.weight is frozen\n",
      "stage_1.2.bn_a.weight is frozen\n",
      "stage_1.2.bn_a.bias is frozen\n",
      "stage_1.2.conv_b.weight is frozen\n",
      "stage_1.2.bn_b.weight is frozen\n",
      "stage_1.2.bn_b.bias is frozen\n",
      "stage_2.0.conv_a.weight is frozen\n",
      "stage_2.0.bn_a.weight is frozen\n",
      "stage_2.0.bn_a.bias is frozen\n",
      "stage_2.0.conv_b.weight is frozen\n",
      "stage_2.0.bn_b.weight is frozen\n",
      "stage_2.0.bn_b.bias is frozen\n",
      "stage_2.0.downsample.conv.weight is frozen\n",
      "stage_2.0.downsample.bn.weight is frozen\n",
      "stage_2.0.downsample.bn.bias is frozen\n",
      "stage_2.1.conv_a.weight is frozen\n",
      "stage_2.1.bn_a.weight is frozen\n",
      "stage_2.1.bn_a.bias is frozen\n",
      "stage_2.1.conv_b.weight is frozen\n",
      "stage_2.1.bn_b.weight is frozen\n",
      "stage_2.1.bn_b.bias is frozen\n",
      "stage_2.2.conv_a.weight is frozen\n",
      "stage_2.2.bn_a.weight is frozen\n",
      "stage_2.2.bn_a.bias is frozen\n",
      "stage_2.2.conv_b.weight is frozen\n",
      "stage_2.2.bn_b.weight is frozen\n",
      "stage_2.2.bn_b.bias is frozen\n",
      "stage_3.0.conv_a.weight is not frozen\n",
      "stage_3.0.bn_a.weight is not frozen\n",
      "stage_3.0.bn_a.bias is not frozen\n",
      "stage_3.0.conv_b.weight is not frozen\n",
      "stage_3.0.bn_b.weight is not frozen\n",
      "stage_3.0.bn_b.bias is not frozen\n",
      "stage_3.0.downsample.conv.weight is not frozen\n",
      "stage_3.0.downsample.bn.weight is not frozen\n",
      "stage_3.0.downsample.bn.bias is not frozen\n",
      "stage_3.1.conv_a.weight is not frozen\n",
      "stage_3.1.bn_a.weight is not frozen\n",
      "stage_3.1.bn_a.bias is not frozen\n",
      "stage_3.1.conv_b.weight is not frozen\n",
      "stage_3.1.bn_b.weight is not frozen\n",
      "stage_3.1.bn_b.bias is not frozen\n",
      "stage_3.2.conv_a.weight is not frozen\n",
      "stage_3.2.bn_a.weight is not frozen\n",
      "stage_3.2.bn_a.bias is not frozen\n",
      "stage_3.2.conv_b.weight is not frozen\n",
      "stage_3.2.bn_b.weight is not frozen\n",
      "stage_3.2.bn_b.bias is not frozen\n",
      "fc.weight is not frozen\n",
      "fc.bias is not frozen\n"
     ]
    }
   ],
   "source": [
    "model.freeze_stage(2)\n",
    "model.print_freeze_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= ------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f'= {\"-\" *30}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = torch.load('w0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del w0['fc.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0['']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
